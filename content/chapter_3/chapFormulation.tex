\chapter{Problem modeling}\label{chap:formulation}
% comit 21-11-2017
\minitoc

%----------- newPlane  ----------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section { The map}
%	\subsection{...}
%	\subsection{...}
%\section{Camera definition}
%
%\section{Cost Function}
%	\subsection{Constraint}
%	


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%The following chapter is focused on the problem formulation for cameras positioning.\\
%Obviously formulation proposed in not the only one and some other have been proposed in the literature, mostly depending then the objectives. \\

%The problem formulation has to translate the objective with all this problematic into a simple formulation usable for different optimization algorithms. 
The following chapter is dedicated to formulate in detail the problems of cameras positioning. 
The objective here is to find the position for a set of cameras or waypoints. The position of this set of waypoints has to be optimized in order to cover most of the area. The area to cover maybe a vast and composed by complex zones.
A good formulation is essential to design an efficient cost function. The cost function is used to quantifies the quality of the solutions. It is a crucial element for the optimisation processes.\\
This chapter present a formal definition of the problem based on the literature and our proposed formulation. 
The formulation proposed is adapted to optimize the problem of cameras or waypoints position depending then many constraint as  a complex and vast map or  cameras mounted on UAV.
%The formulation proposed is adapted to optimize the problems with evolutionary algorithm to have an efficient cameras position for maximizing the coverage, depending then many constraints as for example using a camera mounted on UAV.

%\section{Coverage estimation }


The following sections are focused on how to estimate the covered area depending on the cameras parameters.\\

%What should be control by the video surveillance? \\
%One aim of the video surveillance is to detect some anomaly in the area for that the number of black hole need to be reduce. A black hole is zone not cover by the system of video surveillance. Estimate the part cover by the system of video surveillance is primordial to limit the size and the number of black hole. 
%To know if the area is cover by a minimum of one vision sensor, different techniques can be used.
%Indeed the coverage estimation is essential element to optimise the position of a network of cameras. But before to find the position some assumption should be done to describe our problems and explain the methodology to calculate the coverage rate of an area.
%The main purpose of our work is to estimate the position for the vision sensors network into a given area.\\
%Where the main goal is to manage the positioning of the fix number of cameras in order to maximize the visual coverage. 
%Indeed before to find the position some assumption should be done to describe our problems.\\ 


%To have an efficient optimization, the estimation of the answer are essential. Here the first element to evaluate the answer is the area coverage by a set of cameras mounted on a UAV. \\


%The problem needs to be formalized in order to clearly identify the complexity of the cameras positioning. 

To estimate efficiently the area covered by a given set of cameras some points have to be clearly defined.
First of all the area himself. The question of "How to represent the area?" is  primordial. The camera definition  which the question of  "How to estimate the camera projection?" is also an important part of the problem design.  The third part to take care  is the constraints which must be added to the systems.
%\begin{itemize}
%\item The area himself. How to represent the area. 
%\item The camera definition. How to estimate the cameras projection.
%\item The constraints added to the systems (constraints and secondary objectives). \\
%\end{itemize} 
 This three parts are discussed in the following sections. The area definition is discussed in the section \ref{sec:Grid} dedicate to the grid design. The camera definition is discussed in the section \ref{sec:CamerasCoverage} and the third part discuss about the constraints in the section \ref{sec:constraint}.\\
Finally when the problem is clearly defined all the different elements are integrated to have an efficient cost function usable for the optimisation process in \ref{sec:costFun}.

   
%;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; to do ;;;;;;;;;;;;;;;;;;
%
%The term coverage denote the area visible by at least one camera. Also the area to cover represent all the parts of the area which must be control by at least one camera. \\
%
%The area definition is primordial to the coverage estimation the folowing parts is focus on it . 
%The area covered by the set of the cameras will be the based of the cost function. In the second time the cost function will be updated by adding the constraints due to the systems.\\
% 
%Therefore the problem is formulated as:
%\begin{itemize}
%\item [-] Maximization of the coverage with a fix number of cameras.
%\item [-] Minimization of the constraints.\\( as examples the complex shape of the area, the bound of the area, the altitude of the cameras,...)
%\end{itemize}

%%%%%%%%%%%a
%Due to this formulation, composed by the maximisation and minimization, the solution  have to evolve to a best  possible answer based on the cost function. Depending then the formulation of the area and the design of the cost function the ability to optimize a solution is greatly affected. 
%The positions of the cameras have to limit the number and the size of black
%holes to be exploitable. The black holes are the area not covered by the camera
%views. To do so a precise estimated of the part covered by the system of camera views is elemental.\\
%
%
%In order to estimate the coverage properly the area must be discretized. Once the area discritezed it became easier to verifies if each point of the area are coverer by at least one camera of the network.
%%%%%%%%%%%%%%a
%The coverage problem must be modelled properly. The different method to design the problem will affect the solution and the applicable resolve method. The following part will talk about it \\
%The coverage problem must be modelled properly. The different method to design the problem will affect the solution and the applicable resolve method. \\
%The first part is to estimate properly the coverage of the area. To do so many method have been developed most of them are based on the occupation grid $G$ of the area. \begin{equation}
%G=\begin{bmatrix}
% g_1 ...g_i ... g_m
%\end{bmatrix}  , m\in \mathbb{N}
%\end{equation}
%Where $m$ is the number of points in the grid.
%The occupation grid is mostly placed on the floor of the area to cover. At minima each point $g_i$ of the grid $G$ should be cover by the vision sensors. Consequently a list of points is created to listed the covered part of $G$ which are noted as $Pc$.
%\begin{equation}\label{eq:Pci}
%g_i \in Pc \mbox{ IFF } g_i \mbox{ is coverd. }
%\end{equation}
%
%The modelization of the grid is an important element and different solution has been proposed during the time with different advantage depending then the situation.\\

\section{The map}\label{sec:Grid}


%/!$\backslash$ \textbf{ ajouter une partie dedier  a [181*] qui propose une solution pour ajusté la densité en fonction  de la necesité de présision  (contour plus dense ) }

The first part is tantamount to estimate properly the area to cover. To do so many methods have been developed and most of them are based on an occupation grid $G$ of the area. The occupation grid is a sample discretization of the area with numerous points.
\begin{equation}\label{eq:Grid}
	G=\begin{bmatrix}
	 	g_1 ...g_i ... g_m
	\end{bmatrix}  , m\in \mathbb{N}
\end{equation}
Where $m$ is equal to the number of points in the grid.
The occupation grid is placed on the area to cover. Each point $g_i$ of the grid $G$ should be covered by a camera. In the Figure \ref{fig:cam_projOnGrid} the grid $G$ and the points $g_i$ are represented, as well as one camera and this associate projection on to the grid.
 Consequently some part of the grid is covered (red dot in the Figure \ref{fig:cam_projOnGrid}). 
% Consequently the list of points is set up to enumerate the covered part of $G$ which are noted as $Pc$. 
%\begin{equation}\label{eq:Pci}
%g_i \in Pc \mbox{ IFF } g_i \mbox{ is coverd. }
%\end{equation}
\begin{equation}\label{eq:B}
\forall g_i \in G, B(g_i)= \begin{cases} 1, & \mbox{if } g_i \mbox{ is covered by at least one camera }\\ 0, & \mbox{otherwise}   \end{cases}
\end{equation}
$B$ is the set of grid points, which are at "1"  if covered by at least one of the camera. The size of $B$ is related to the size of $G$.
\begin{figure*}[t!]
		\centering
		\minipage{0.85\textwidth}
  		\includegraphics[width=\linewidth]{img/CamProjectOnGrid2.png}
  
 	 	\endminipage\hfill\caption{Camera projection onto a grid. The grid $G$ is placed on the floor to discretize the area covered with numerous grid points $g_i$. The point covered by the camera $B()$ are noted in red.}\label{fig:cam_projOnGrid}
\end{figure*}
\\ The design of the grid is an important element of the problem formulation. Many solution has been proposed, with different advantage depending on the situation (constraint and objective).\\

\subsection{How to design a grid map} \label{sec:GridMap}%Related work for the grid design

The following section is focused on the different grids design, based on the literature. The use of grid to describe the area to cover is common and several design has been invented and used depending then the constraints and objectives.  


\subsubsection{Sampling frequency} %Sampling frequency.\\
The grid map is used to discretize the area to cover. The discretization of the area may vary and the area can get a high level of discretization or low level. The level of sampling frequency has a bearing both on the problem formulation and on the optimization process.

\paragraph*{High sampling frequency}
%The high level of discretization or high sampling frequency has some advantage and disadvantage.\\ 
The high level of discretization or the high sampling frequency of an area is characterized by a big amount of point $g_i$ to describing the area. The big amount mean to have an important density of point $g_i$ and consequently the value of $m$ is high. 

%\begin{equation}
%	G=\begin{bmatrix}
% 		g_1 ...g_i ... g_m
%	\end{bmatrix}  , m\in \mathbb{N}
%\end{equation}
%
%Where m is the number of point into the grid.
% \subparagraph{Advantage of the  high sampling frequency.}
%////////////high sampling advantage////
The main advantage, is to have a better estimation of the coverage. More the area is finely discretize more the estimation of the coverage will be sharp. In \citep{171*horster2006} an example of high frequency sampling is given in order to have sharp estimation of the area. \\
The high sampling frequency allows the cameras position to be much more accurate and make a very small adjustment.
%////////////high sampling disadvantage////\\
\\On another side, a too high sampling frequency lead to high computation times. Rather to refine the solution the too high level of discretization of the area will make the optimization too long and more complex. Indeed to control the coverage, it is necessary to control if each point of the grid is covered by a camera.  
%\begin{equation} 
%	\sum^m \sum^n \mbox{test of coverage}
%\end{equation}
%Where $m$ is the number of points in the grid and $n$ the number of camera. 
%That mean the number of test to estimate the coverage of each point of the grid, for a given set of cameras is $m\times n$. Where $m$ is the number of points in the grid and $n$ the number of camera. \\
 More the size of the grid is high, more the unity test of coverage (see in Section \ref{sec:CamerasCoverage}) must be done, and that at each step of the optimization process. Consequently the size of the grid will greatly affect the time computation.  \\
Also the high sampling frequency will affect the positioning of the cameras pose. In fact, higher is the sampling frequency of the area  more freedom has to be given to the  pose estimation of  each camera.


%////////////////////////////////////

%In fact the quality of the coverage estimation affect the positioning optimization. %A bad estimation of the area coverage due to the bad level of discretization will impact the quality of the answer and the precision of the cameras position by returned a approximate coverage rate. 
\paragraph*{Low sampling frequency}
%///////////////////advantage of the low sampling///////
At the opposite a lower sampling frequency can be a good solution to upgrade the convergence speed of the optimization process as that was presented in \cite{8*zhou2011}. In Zhou et al \cite{8*zhou2011} a small value of $m$ is chosen to have a real time solution for small area with just few cameras (up to  20). 
%////////////low sampling disadvantage////
On the other side, the low sampling frequency may generate a bad estimation of the area covered due to the too low density of points in the grid. The low density of points may give an approximated view of the area covered and some uncovered area (black hole) can appear between the points of the grid. This uncovered area can be too small to be detected. Due to the low density of points. In this case, the optimization cannot take in to account this uncovered area and the solution given after an optimisation will be in a real environment not good as aspect. Concretely, the difference between the coverage estimation of the discrete area and the  coverage estimation in continue must be high.    \\



%////////////////////sumup//////////

% tableau auto généré   bug 
%\begin{table}[]
%\centering
%\caption{My caption}
%\label{my-label}
%\begin{tabular}{l|l|l|}
%\cline{2-3}
%\textbf{} & \textbf{Avantage} & \textbf{Disadvantage} \\ \hline
%\rowcolor[HTML]{FFFFFF} 
%\multicolumn{1}{|l|}{\cellcolor[HTML]{FFFFFF}} & Best estimation of the area to cover & \cellcolor[HTML]{FFFFFF} \\
%\multicolumn{1}{|l|}{\multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}\textbf{High sampling frequency}}} & Give more precision on the cameras poses & \multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}Time consuming} \\ \hline
%\rowcolor[HTML]{EFEFEF} 
%\multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}\textbf{Low sampling frequency}} & Faster computation & Bad coverage estimation \\ \hline
%\end{tabular}
%\end{table}
\paragraph*{Low or high sampling frequency}
Finally, the too low sampling frequency, instead to win computation time, may affect the quality of coverage  estimation. But in the contrary, the impact of a too high sampling frequency has consequences, as is summarized in the Table \ref{tab:samplingFrequency}. The choice of the sampling frequency have to be done carefully. \\
The density of the grid has to be adjusted depending than the goal and the precision required. One of the solutions proposed in Zhao et al \citep{22*zhao2008} is to have a progressive refinement by increasing the grid density.
%////////////progressive sampling/////
Zhao et al \citep{22*zhao2008} despite a low density of points at the beginning, the number of points are increased slowly to have a better density and refined results. Also the increasing points frequency is applied to refine the solution and add more cameras at each step of the optimisation to avoid the uncovered area.  \\


%////////////camera pose estimation linked//////////////////////
%!!!!!!!!!!!!!!!!move it somewhere!!!!!!!!!!!!!!!!!!!
\subsubsection{Cameras pose precision}
The frequency sampling of the grid is often linked to the pose precision of the camera as discussed previously. In some case as in  \citep{83*van2009,150*chakrabarty2002,195*choi2009,87*morsly2012}, the available position of the cameras are strongly linked to the grid map and the sampling frequency.
 %The search space represent the domain to optimize or can be defined by the set of all possible solutions in our case the search space is all the position for all the cameras. 
Increase the number of points in the grid will also increase in proportion the number of possible positions for each cameras. More the area is finely defined more it is necessary to can slightly adjust the cameras positions. Consequently the possible camera position and the search space are increased to finally allow a more refine solution but also more complex to optimize.

On the other hand, the cameras poses can be limited at some specified area of the map. In \citep{171*horster2006,22*zhao2008} the cameras can be placed against the walls. In \cite{22*zhao2008} every wall  of the boundary can hold a camera. In \cite{171*horster2006} the camera can be placed only on the specified zone (see the   in the Figure \ref{fig:randomGridRef171}.d, the green wall shown the available position of the cameras).

The cameras pose precision has a similar impact and consequences on the result then  for the gird map.
A to big  number of possible pose allow to gain in precision but greatly increase the complexity, where a too small  number of possible pose help to converge faster but with a bad precision. 
 %151*
 %The too high precision in the position of the camera will increase the complexity and some case where the value of $m$ is too high the optimization process cannot converge or find acceptable solution to the gain of the search space. \\ 
%\\!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
% 171*  cam position sur les mure  see fig 4.3
%22*   came position on the wall
%151* cam pos wall 
%8*  camera  free position low sampling F
%83*  camera position is a miroring of the grid map
%150*  capteur pose topologique 
%195*  hover robot   218
%215*  hover roboto exploration 
\subsubsection{Distribution}
The distribution of the points over the grid, is an important factor to manage for the design of an occupation grid.% The distribution is how the points of the grid are placed on the area to cover.
The points distribution can numerous and adapted to different specific case. Mainly two distribution are used. The grid pattern distribution is the more often seen in the literature and in a lesser extent the random distribution. These two distribution  are illustrated in the Figure \figref{fig:GridVsRand}.

%Different distributions can be used, but commonly in the literature the grid pattern distribution and in a lesser extent the random distribution is applied (see Figure \figref{fig:GridVsRand}). 
\begin{mfigures}[!]
{Map representation using random distribution or uniform grid.}{fig:GridVsRand} \centering
\mfigure{width=.4\linewidth}{img/GridVsRand1.png}{Grid with uniform and regular distribution.}{subfig:satimgTorcy}
\hspace{1cm}
\mfigure{width=.4\linewidth}{img/GridVsRand2.png}{Random distribution for represent the map.}{subfig:satimgMaskTorcy}
\end{mfigures}

%\begin{figure}[t!]
%\begin{center}
%\minipage{0.65\textwidth}
%   \includegraphics[width=\linewidth]{img/GridVsRand.png}
%  \caption{ (a)Grid with uniform and regular distribution.\\   
%(b) Random distribution.\\  
%}\label{fig:GridVsRand}
%  \endminipage\hfill
%  \end{center}
%\end{figure}
%%%%% FIGURE  random distribution  form 171*
\begin{figure}[t!]
\begin{center}
\minipage{0.65\textwidth}
   \includegraphics[width=\linewidth]{img/randomGridRef171.png}
  \caption{Example of  map to cover with a randomized area sampling. Illustration form Horster et al \citep{171*horster2006} \\
  (a) Area  to cover.\\ 
  (b) Area with the random points of the grid to cover \\
  (c) Importance weighting of the area (zone of the interest.\\  
  (b) allowed position for cameras position.}\label{fig:randomGridRef171}
  \endminipage\hfill
  \end{center}
\end{figure}

 The random distribution is used to describe the area to cover as  for exemple in \cite{83*van2009,171*horster2006}. \\
In Hoster et al \citep{171*horster2006} the points of the grid are randomly distributed to describe all the area to cover. The advantage of this article is to use the random distribution in order to manage the density of points in some specific region of the area. Especially, by increasing the density of points on some specific zones of the area. The increased density allocates more importance to these zones (see Figure \ref{fig:randomGridRef171}).\\
Indeed the higher density will affect the optimization process. The area with more density will be comparatively more profitable to a low density area. In these cases, the zones with high density are covered in priority. This mechanism is even simplified due to the random distribution.

The hengel et al \cite{83*van2009} have tested the random distribution and the uniform grid pattern distribution before to conclude the grid pattern and the random distribution proposes globally the same result, when there is no priority zones in the area. Based on this observation author (hengel et al \cite{83*van2009}) decided to use the uniform grid because of its simplicity of implementation.
%///////random distribution   for reduce the uniforme grid //////////
A  hybrid distribution is proposed is proposed in  Zhao et al \cite{22*zhao2008}.
%The "random distribution" is less popular but in  a hybrid distribution is proposed. 
The idea is to reduce the number of points in the uniform grid when it has a to hight density. To reduce the number of points in the grid a random selection is used. This hybrid implementation of a uniform grid with a random selection to decrease the sampling frequency are an example of hybrid distribution.


\subsubsection{Spatial modelling (3D or 2D)}
 The spatial modelling,  has an important impact on the grid design. After deciding the useful density and the distribution of the grid, the position in the space of the points $g_i$ which compose the grid  has to be discussed.  
%In fact depending on the problem the grid can be modelled differently in order to properly cover the area to control.
 Commonly the occupation grid is placed on the floor and calculate visibility only in 2D  by computing the  camera projection as in  \citep{164*valente2013,150*chakrabarty2002,8*zhou2011,170*yabuta2008,171*horster2006,22*zhao2008}, but depending on the context the grid has interest do describe a 3D space.  
Hengel et al \cite{83*van2009} calculates the visibility, where it is relevant: for example, on the upper torso or head of the possible target rather than the floor. In this case a 2D grid is proposed, but inside the 3D space in order to  characterize properly the volume by placing the grid at a specific height.  

In Chrysostomou et al \cite{82*chrysostomou2012} or also in \cite{226*nazarzehi2018}, the grid is formalized in the full volumetric space by numerous “control points” to control the area (illustrated in Figure \ref{fig:3Dgridfrom226}). The points of the grid are uniformly distributed along the axis of $x, y$ and $z$. The grid are formulated in the volumetric space by using a uniform 3D grid distribution.
The uniform grid distribution in a 3D space increase greatly the complexity  due to the important augmentation of control point ($g_i$) and their dispersion. His implementation is unusual due to the increasing difficulty for the optimization process. 
To avoid the increasing complexity the 3D grid is replaced by start of  2D grid at a specific height to optimize the coverage. 
 %The formulation proposed shown the complexity even more important to use a 3 dimensional occupation grid. For practical reasons, mostly the inadequacy has computational power due to the increased complexity, the 3D grid is replaced by a 2D grid at a specific height to optimize the coverage. 
%While an occupation grid designed to estimate the volume cover along the $ x, y $ and $ z $ axes of the area already exist. %His implementation is unusual due to the increasing difficulty of the optimization process.  
Nevertheless some solution was discussed \citep{141*akbarzadeh2013,83*van2009} to take into account the volume of the area to cover which cannot be only limited to an occupation grid along the axes $ x $ and $ y $ as a simple 2D grid.
\begin{figure}[t!]
\begin{center}
\minipage{0.65\textwidth}
   \includegraphics[width=\linewidth]{img/3DgridFrom226.png}
  \caption{3D grid with uniform distribution of the grid point to cover. Illustration from \cite{226*nazarzehi2018} }\label{fig:3Dgridfrom226}
  \endminipage\hfill
  \end{center}
\end{figure}

In  Hengel et al\cite{83*van2009} the authors are focussed on estimating the area to cover inside a 3 levels of a building. To estimate the coverage in the building the grid has been placed at each level. This solution is efficient in order to limit the number of grid points compares then a full volumetric description of the area. Also this design allows to keep the 3 dimensional informations by adding a layer at each level of the building (see Figure \figref{fig:gridByLevel83}). 
 %*** from 83 ***
 \begin{mfigures}[!]{Illustration of a grid layer in 3 level building from \citep{83*van2009}. In the figure (b) (d) (c) the uniform grid is visible with the black case of the grid represent the area uncovered and the colored case for the covered area.}{fig:gridByLevel83} \centering

\mfigure{width=.4\linewidth}{img/buldingViewfrom83.png}{Building view.}{subfig:r4}
\hspace{1cm}
\mfigure{width=.4\linewidth}{img/Level1from83.png}{1$^{st}$ floor.}{subfig:r1}
\hspace{1cm}
\mfigure{width=.4\linewidth}{img/Level2from83.png}{2$^{nd}$ floor.}{subfig:r2}
\hspace{1cm}
\mfigure{width=.4\linewidth}{img/Level2from83.png}{3$^{rd}$ floor.}{subfig:r3}


\end{mfigures} 

Another way to deal with a volumetric space with not using a greedy 3D grid distribution is to adapt the 2D grid into the relief. In Akbarzadeh et al\citep{141*akbarzadeh2013} propose a 2D grid adapted to the relief of the area. This article are focused on covering a large outside area with an important relief (hill and valley). To estimate properly the area to cover a grid has been placed following the altitude of the relief as showed in Figure \ref{fig:grilleRef141}. The spatial modelling is traditionally a 2D grid at fix altitude more or less equal to the floor ground. 
The 2D grid distribution adapted to a 3D environment is the easiest and can be customize for numerous problems with allow to reduce the number of control points ($g_i$) and thus  the computation time.  

%%% figurir  from 141*
\begin{figure}[t!]
\begin{center}
\minipage{0.65\textwidth}
   \includegraphics[width=\linewidth]{img/grilleRef141.png}
  \caption{Relief grid use to discretize  an area with taking in account the relief. Illustration from \cite{141*akbarzadeh2013} }\label{fig:grilleRef141}
  \endminipage\hfill
  \end{center}
\end{figure}

\subsubsection{Zones of interest} \label{sec:zoneOfInterest}
Among the area to cover, some zones may have a particular interest to be covered. These zones can be discriminated, by the grid design. 
Mainly 3 methods can be discerned to highlight the zones of interest:
\begin{itemize}
	\item[-]	The multi coverage zones. 
	\item[-]	The priority zones.  
	\item[-]	Non-interesting zones.
\end{itemize}
 

\subparagraph{The multi coverage zones}
The aim of the multi coverage zones, is to have on a specific zone of the area controlled by numerous cameras.  Multiple coverage may be called $k$-coverage as in \cite{174*zhang2016}. Where refers $k$ to the number of cameras mandatory to cover the zones of interest. 
Every points of the grid $G$ should be covered by at least one camera and for some specific zone of the area by $k$ cameras.\\
 %Consequently a binary list of points is created to count the covered part of the grid $G$ which are noted as $Pc$.

\begin{equation}\label{eq:PciK}
\forall g_i \in G, B^{(k)}(g_i)= \begin{cases} 1, & \mbox{if } g_i\mbox{ is covered by $k_i$ cameras, } k_i \in K \\ 0, & \mbox{otherwise}   \end{cases}
\end{equation}
Where $k_i$ is the number of cameras used to cover the point $g_i$ of the grid. $K$ is the list of $k_i$ associate to the number of points in the grid. The list $K$ is initialized at one by default, excepted for the multi coverage zones,  where they have to be covered  by $k$ cameras as in \cite{82*chrysostomou2012}. 

\subparagraph{The priority zones}
The zones to cover in priority are used especially in the case where the number of cameras are not sufficient to fully cover the area. This priority of coverage can be expressed by different ways. \\
One way is to express  the priority with a grid uniformly distributed, where a  weighting on the points of the grid which need to be covered in priority is applied, as in \cite{141*akbarzadeh2013,84*xu2011}.
 This method was implemented in \citep{141*akbarzadeh2013} to optimize the position of the camera on the road passing through the area to cover. \\
%In \citep{171*horster2006} the weighting of the priority zone is made by increasing the sampling frequency of the zones thanks to random distribution  as in Figure \ref{fig:randomGridRef171}(b).
Another way to  express the  weighting of the priority zone  for  the randomly distributed points, is  to increase the sampling frequency of the desired zones. Increase the sampling frequency of the priority zones is simplified due to the use of a random distribution as in Horster et al \citep{171*horster2006} and as Illustrate in the Figure \ref{fig:randomGridRef171}b.
 Using this method the zones of interest are more dense and that push the optimization process to cover this area in priority (more density means more interest).\\
Otherwise the priority zone in the uniform grid distribution can be formulated as: 
  \begin{equation}\label{eq:PciP}
\forall g_i \in G, B^{(p)}(g_i)= \begin{cases} 1*p_i, & \mbox{if } g_i\mbox{ is covered and  $p_i$ is the weight, }p_i \in P  \\ 0, & \mbox{otherwise}  \end{cases}
\end{equation}
Where $p_i$ is the weight of the point $g_i$ on the grid $G$. $p_i \in P $ is the list of $p_i$ which contain the weighting of the area associate to the points of the grid $g_i$ which have to be covered in priority. \\

\subparagraph{Non-interesting zones}\label{subsec:obstacleZone}
Non-interesting zones are as it name suggests, the zones without interest to be covered by the set of cameras, noted $U$. The set $U$ is composed by the list of points $g_i$ with have no to be covered. These zones are not strongly prohibited. That mean the zones considering as non-interesting can be covered but their coverage or un-coverage has no impact on the coverage estimation of the area. In the case of random grid, distribution the non-interesting zones have a very low sampling frequency or null as in \citep{141*akbarzadeh2013}. For uniform grid distribution, the non-interesting zones are removed to the list $G$ in order to create a simplified list $G'$. This method is currently used like in \cite{22*zhao2008,170*yabuta2008,141*akbarzadeh2013,171*horster2006,84*xu2011}. 
\begin{equation}\label{eq:setU}
G'=G-U \mbox{ ,    }  \mbox{ }U= \{ g_i | g_i \in G, g_i \mbox{ are the non intresting points} \}
\end{equation}
Instead of defining the non-interesting zones as a set $G$ that excludes $U$ to have a smaller set $G'$, the non-interesting zones can be defined as priority zone with some $p$ equal to $0$ (in Equation \ref{eq:PciP}). This way of defining the non-interesting zones is more greedy in computation and so the formulation in Equation \ref{eq:setU} it is preferred  to the  0 weight of $p$ (Equation \ref{eq:PciP}). 

Finally, these methods (Equation \ref{eq:setU}) use to design the zones of interest are not fully independent and can be associated with the same model as in \cite{141*akbarzadeh2013,171*horster2006,84*xu2011}. The combination of all these zones of interest can be formulated as : 
 \begin{align}\label{eq:PcFull}
\forall g_i \in G', B^{(k,p)}(g_i)= \begin{cases} 1*p_i, & \mbox{if } g_i\mbox{ is covered  by $k_i$ and  $p_i$ is the weight, } p_i \in P \mbox{ and } k_i \in K \\ 0, & \mbox{otherwise}  \end{cases}
%\\ G'=G-U \mbox{ ,    }  \mbox{ }U= \{ g_i | g_i \in G, g_i \mbox{ are the non intresting points} \}
\end{align}
Where $G'$ is the restricted point of list to cover which take  in account the removal of the non-interesting zones (as in Equation \ref{eq:setU}). $P$ and $K$ are respectively the priority zones and the  multi coverage zones.
   
\subsubsection{Atypical design}  

Previously the method to set-up a classical occupation grid depending on the problem has been discussed. Few other solutions more atypical have been developed. One solution coming from the field of wireless sensor network is the topologies grid. 
 The topologies grid is clearly explained in Chakraborty et al \cite{150*chakrabarty2002}. The interest of these methodologies is to reduce the number of points to cover. But in this case the number of points is not related to the resolution wished  but by the sensor range. Indeed the size between the points of the grid has been defined by the size of the minimum sensors range. The distance of the minimum sensor range are used as nodes for a topological relation. \\
Another atypical solution is to develop a grid composed by rectangles. Each rectangle may have different sizes adapted to the obstacles in the area. A rectangle is considered as covered if most of the area of the rectangle is covered by the cameras as in \citep{170*yabuta2008}. This method is adapted to the area with few obstacles as is shown in the Figure \ref{fig:from170}.
\begin{figure}[t!]
\begin{center}
\minipage{0.65\textwidth}
   \includegraphics[width=\linewidth]{img/from170.png}
  \caption{Observed regions from camera candidate. Grid representaiton from \citep{170*yabuta2008}.}\label{fig:from170}
  \endminipage\hfill
  \end{center}
\end{figure}

\newcolumntype{M}[6]{>{\raggedright}m{#1}}

\subsubsection{ Grid map design sum-up }

%The following paragraph surmise the different ways presented to design the map of the area. The area represented has to be covered by set of cameras.
 The grid map may be used to represent different objectives  and constraints of the problems. The map representation take an crucial role in the area coverage. The design aborted previously are summarized here in this Table \ref{tab:mapSumUp}. This table summarize the more important aspect of each grid representation in different papers.



% \begin{center}
%   \begin{tabular}{ l | m{0.1\linewidth} | m{0.1\linewidth} | m{0.187\linewidth} | m{0.147\linewidth} | m{0.115\linewidth}   }
%     \hline
%     ref & grid pattern & grid random  & volumetric space & Sampling frequency &Zone of interest \tabularnewline \hline 
%	 \cite{8*zhou2011} zhou2011  		&	 X	 &	     & 	   						  & For real-time    & X \tabularnewline   \hline 	
%	 \cite{22*zhao2008} zhao2008 		&	 X	 &	 X   & 	   						  & Incremental	 	 & X \tabularnewline \hline     
%	 \cite{82*chrysostomou2012} chrysostomou2012 &	 X	 &	     &	3D grid					  &  				 & X \tabularnewline \hline
%       \citep{83*van2009} van2009 		&	 X	 &	 X   & 	2D grid at each level  	  &  				 &	\tabularnewline \hline
%    \cite{87*morsly2012} morsly2012	 	&	 X	 &	     &  Superposition of 2D grid  &  	Adaptable	 &	\tabularnewline \hline
%\cite{141*akbarzadeh2013} akbarzadeh2013       &	 X	 &	     & 	2D grid on relief 		  &  		 		 & X \tabularnewline \hline
%   \cite{150*chakrabarty2002} chakrabarty2002   &	 X	 &	     & 	  	 					  & Topologies sensor&	\tabularnewline \hline
%     \cite{164*valente2013} valente2013
%     &	 X	 	&	     &overlap by shifting of z  &  				 &	\tabularnewline \hline
%      \cite{170*yabuta2008} yabuta2008     &	 .	 & 	     &		  					  &For zone 
%     															segmentation 	 & X \tabularnewline \hline
%      \cite{171*horster2006}    &	  	 &	 X   & 	  	 					  &  		 	 	 & X \tabularnewline \hline
%      \cite{174*zhang2016}      &	  	 	&	     & 	 	 					  & 				 & X \tabularnewline \hline
%           
%   \end{tabular}
% \end{center}
%	 

\begin{landscape}
\begin{table}[!h]

\begin{tikzpicture}[right]
\node (a) at (0,0)
{

\begin{tabular}{|c|}
\hline
  \textbf{Zone of  interest} \\ \hline
   \textbf{Sampling frequency}   \\\hline %\vdots
  \textbf{Volumetric space}  \\\hline
   \textbf{Grid random }\\ \hline
   \textbf{Grid pattern}  \\ \hline
\end{tabular}
};

\node[yshift=-4.789cm,xshift=-1cm] (b) at (a.south) 
{
\begin{tabular}{l | m{0.031\linewidth} | m{0.024\linewidth} | m{0.255\linewidth} | m{0.18\linewidth} | m{0.1631\linewidth} }
\hline
\toprule

\rowcolor[HTML]{FFFFFF} 
\multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}\cite{8*zhou2011} Zhou2011}   		&	 %\ding{52} 
	 &	     & 	   				2D gird		  & For real-time    &  \ding{55} \tabularnewline    	
\rowcolor[HTML]{EFEFEF} 
\multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF} \cite{22*zhao2008} Zhao2008}  		&	  \ding{52}	 &	  \ding{52}   & 	   	2D gird at torso hight	  & Incremental	 	 &  Non-interesting zones  \tabularnewline \hline     
 \rowcolor[HTML]{FFFFFF} 
\multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}\cite{82*chrysostomou2012} Chrysostomou2012}  &	  \ding{52}	 &	     &	3D grid for volumetric space  &  	Non specified			 &  Non-interesting zones  $\&$ possible k-coverage \tabularnewline 
\rowcolor[HTML]{EFEFEF} 
\multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF} \citep{83*van2009} Hengel2009 } 		&	  \ding{52}	 &	  \ding{52}  & 	2D grid at torso hight for each level  of a bulding  &  	Hight sampling	 & Non-interesting zones  $\&$ k-coverage	\tabularnewline 
  \rowcolor[HTML]{FFFFFF} 
\multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}\cite{87*morsly2012} Morsly2012} 	 	&	  \ding{52}	 &  & 2D grid  &  	Adaptable	 &   \ding{55}	\tabularnewline 
\rowcolor[HTML]{EFEFEF} 
\multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF} \cite{141*akbarzadeh2013} Akbarzadeh2013 }      &	  \ding{52}	 &	     & 	2D grid on the relief (Figure \ref{fig:grilleRef141})  &  	Very hight sampling 		 &  Non-interesting zones  $\&$ priority coverage \tabularnewline 
  \rowcolor[HTML]{FFFFFF} 
\multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF} \cite{150*chakrabarty2002} Chakrabarty2002}    &	  \ding{52}	 &	     & 	  	 	2D grid		  & Topologies sensor (low sampling) & \ding{55}	\tabularnewline 
\rowcolor[HTML]{EFEFEF} 
\multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF} \cite{164*valente2013} Valente2013}       &	  \ding{52}	 	&	     & 2D grid with overlap by shifting of z  &  	Low sampling frequancy			 &	\ding{55}\tabularnewline 
   \rowcolor[HTML]{FFFFFF} 
\multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF} \cite{170*yabuta2008} Yabuta2008}         &	  \ding{72}	 & 	     &		  	2D grid (Figure \ref{fig:from170})	  & Grid paterne at initialisation then zones segmented  & Non-interesting zones  $\&$ priority coverage \tabularnewline 
\rowcolor[HTML]{EFEFEF} 
\multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF} \cite{171*horster2006}  Horster2006}   &	  	 &	  \ding{52}   & 	  	 	2D grid (Figure \ref{fig:randomGridRef171})		  &  	Hight sampling 	 	 	 &  Non-interesting zones  $\&$ priority coverage \tabularnewline \hline
   \rowcolor[HTML]{FFFFFF} 
%\multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}\cite{174*zhang2016}  Zhang2016}    &	  \ding{52}	 	&	     & 	 	 			2D grid   & 				 &  \ding{52} \tabularnewline \hline

    	 
\end{tabular}
};
%\draw[->,ultra thick](a)--(b);
\draw [->,very thick] (4.25,1.02) -- (21,1.02) -- (21,-2);
\draw [->,very thick] (4.25,0.51) -- (16,0.51) -- (16,-2);
\draw [->,very thick] (4.25,0.0) -- (11,0.) -- (11,-2);
\draw [->,very thick] (4.25,-0.51) -- (7.5,-0.5) -- (7.5,-2);
\draw [->,very thick] (4.25,-1.02) -- (6.2,-1.02) -- (6.2,-2);

%\draw [->,very thick] (-1,0) -- (1,0) -- (1,-2);
%\draw [->,very thick] (-1,-0.5) -- (0,-0.5) -- (0,-2);
\end{tikzpicture}
 \caption{Sum-up of the grid map.}\label{tab:mapSumUp}
\end{table}	
\end{landscape}
 
\subsection{Our approach}
 Based on the designs studied the one finally adopted is a grid $G$ as in Eq \ref{eq:Grid} with an uniform repartition following the 2D grid pattern. The grid pattern has been selected due to this facility of implementation and  flexibility to the additional constraints.
  The frequency adopted is fixed depending on the size of the area, the precision required and the cameras properties. The frequency adopted as to be considered as dense (high sampling frequency).
  Also, due to the high sampling frequency the 2D grid pattern is a better choice and allow a reasonable number of points to describe a complex area. 
  The grid is placed on the floor of the area to control. Floor is always considered as flat without relief.
  In our case the zones of interest can be varied and may include non-interesting zones, priorities zones  and  multi coverage. \\
  To can possibly allow all these zones of interest feature, the selected design of the grid is based on the formulation form the Eq:\ref{eq:PcFull}. During the experiment presented in the following  chapters the priorities zones and multi coverage are not exploited. Consequently the $k=1$ and $p=1$.  Although some rapid tests were carried out with k-coverage and priority zone constraints, the results obtained were judged to be of little relevance, hence the choice to focus on constant priorities and single camera coverage for the entire area to be covered.
 Although some rapid tests were carried out with constraints of k-coverage and priority zones, the results obtained were considered irrelevant hence the choice to focus on constant priorities and single camera coverage for the entire area ($k=1$ and $p=1$).
%   The zone of interest can vary depending on the need of the experimentation, but the design chosen is flexible to apply if necessary the formulation form the Eq:\ref{eq:PcFull} with mostly $k=1$ and $p=1$, also a set $U$ is used to represent the non-interesting zone as it was presented in eq \ref{eq:setU}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Cameras coverage}\label{sec:CamerasCoverage}


Once the area to cover is described by the grid, the next step is to verify for each point of the grid if one or more cameras cover it, based on Eq: \ref{eq:PcFull} with $k=1$ and $p=1$.\\
To verify if each points of the grid is covered by a camera. It is primordial to discuses the camera properties and introduce the projection model.
%to talk about what is a camera, what kind of cameras are appropriate and their projection model. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Camera definition}\label{sec:CamerasDefinition}

The perspective projection is often use to define a camera. The camera projection has the advantage to be anamorphic 
%The closest projection model to the human view is the perspective projection. 
Thus the perspective projection is also the more common and more especially in the field of area coverage as in examples \cite{101*topcuoglu2009,33*reddy2012,8*zhou2011,82*chrysostomou2012,22*zhao2008}. Other model of cameras or vision sensor can be used as for example omnidirectional with a $360^{\circ}$ of field of view as \citep{43*erdem2006,150*chakrabarty2002,174*zhang2016}. 
In the following chapters we are only focussing on the camera perspective due to its wide use and it is also the most suitable to be embedded in the UAV for path planning. 
\begin{figure}[t!]
\begin{center}
 \minipage{0.65\textwidth}
   \includegraphics[width=\linewidth]{img/PinholeCam.png}
  \caption{Pinhole camera model.}\label{fig:cameraObscura}
  \endminipage\hfill
\end{center}
\end{figure} 
\\The pinhole or in Latin the "camera obscura" (see Figure\ref{fig:cameraObscura}) is at the origin of the geometry model for the perspective projection.\\
The pinhole model is commonly composed by a box (or  a chamber) hermetically closed to light, excepted by a small pinhole on the middle of the front side. All the ray of light reflected by the objects of the world and passing by the small hole are projected onto the back side of the box. Each ray of light passing by the hole is  projected onto the plan ( the back side of the inside box). This plan became the reversed image of the world and can be recorded by a film or a digital sensor. 
 Due to the simplicity of the pinhole model, the calibration and camera projection estimation is simplified. To estimate the projection only few element has to be knew:
%  Based on it, the camera projection is dependent then few parameters intrinsic (as focal length, sensor size,...) and extrinsic (as the position $x,y,z$ and orientation $\alpha,\beta,\gamma$). 
  
 
\begin{figure}[t!]
\begin{center}
\minipage{0.65\textwidth}
   \includegraphics[width=\linewidth]{img/PanTiltRoll.png}
  \caption{The rotation composed by 3 degrees of freedom on pan tilt roll$(\alpha,\beta,\gamma)$.}\label{fig:PanTiltRoll}
  \endminipage\hfill
  \end{center}
\end{figure}

%In fact the useful elements to define the projection of a camera perspective, can be stated as a set of parameters composed by:\\
\begin{itemize}
\item Three degrees of freedom for the camera position : $(x, y, z)$;
\item Three degrees of freedom for the camera orientation : pan, tilt, and roll angles: $(\alpha,\beta, \gamma)$ 
\item Optical parameters including: the focal length $f$ of the lens, %{the sensor size $Sw\times Sh$,
 $u_{0}$ and $v_0 $  which would be ideally in the centre of the image. $\sigma_{uv}$ represents the skew coefficient between the $x$ and the $y$ axis.
\end{itemize}
%$s$ is composed by two elements $Sw$ and $Sh$ for the size of width and height (also called  the scale factor).\\

Among the parameters of the camera, only  some of them are useful to estimate the projection. They can be formalized as a vector:
\begin{equation}\label{eq:v}
v=(x,y,z,\alpha ,\beta,\gamma,f)%,u_0,v_0,\sigma_{uv} %v=(x,y,z,\alpha ,\beta,\gamma,f,Sw\times Sh)
\end{equation}

Each element of the vector $v$ are used to compute the camera projection on the discretized floor (the grid $G$). 
\iffalse 
To model the set of the cameras using the precedent notation, a set $V$ composed by $N$ cameras defined by $v$ noted:
\begin{equation}\label{eq:V}
\begin{split}
V= \{v_i\} \mbox{  , } \forall i=[1;N] \mbox{ , } N\in \mathbb{N}^*
\\
\mbox{ and } v_i= (x_i,y_i,z_i,\alpha_i ,\beta_i,\gamma_i,f_i,s_i,{u_0}_i,{v_0}_i,{\sigma_{uv}}_i)
\end{split}
\end{equation}
\noindent Where $N$ is the given number of cameras in the solution. Which is also the number of cameras in the set $V$. \\
Therefore, $V$ represent a camera networks and also a initial set of parameter for a set of cameras. $V$ can contain all the possibility, included the non-acceptable answer. A non acceptable  answer will be an answer with  doe's not respect the constraint of the problem.   
 \\ !!!!!!\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
 
    
The hole in the box is called the projection center. It is the point were each ray of light are crossing together.  \\
Based on this model, a 3D point $^CP=(^Cx,^Cy,^Cz,1)^T$ defined in the pinhole reference noted $ ^C\Re$ can be convert in to a 2D point  $^rp=(^ru,^rv,1)^T$ in the sensor reference (back side of the box) by using  the perspective projection model $pr() $: 
\begin{equation}
^rp=(^ru,^rv)= pr(^CP) \mbox{ with } \begin{cases} u= f.\frac{^Cx}{^Cz} \\  v= f.\frac{^Cy}{^Cz} 
\end{cases} 
\end{equation}
Where $f$ is the focal length (distance between the projection center and the photosensitive sensor )
the point $^rp$ is the projected point $^CP$ on the sensor. The sensor is composed by $sw$ pixel of width and $sh$  pixel of  heigh. To convert the coordinate $^rp$ of p in the pixel  that mean in the image reference $^i\Re$ : 
\begin{equation}
^ip=K. ^rp
\end{equation}
where $^ip$  is the point $p$ in the image reference composed by $(u,v) \in ^i\Re$ with $u$ and $v$ are  pixel coordinate. 


%Where $u_0$ and $v_0$ are the coordinate  of  the principal point of the image ( projection of the optic centre on to the image plan).  
%$k_u$ and $k_v$  are the magnification factors of the image on width and heigh. 

% resource http://ksimek.github.io/2013/08/13/intrinsic/
%https://jeux.developpez.com/tutoriels/OpenGL-ogldev/tutoriel-12-projection-perspective/
The cameras is called calibrated when the intrinsic parameters ($K$)are defined.  
 The intrinsic matrix $K$ is parametrized by Hartley and Zisserman and it is composed permit to represent the properties of the camera. To design the intrinsic matrix $K$ some part of the pinhole camera have to be defined :\\
  \begin{itemize}
  
 	\item $f$ is  the focal length. The focal length is the distance between the pinhole and the sensor (a.k.a. image plane)
  	\item $k_u et k_v$ are the magnification factors of the image on width and heigh ; related to the image sensor format%  les facteurs  aggrandisement de l'image 
  	\item $u_0 v_0$  are the coordinate  of  the principal point of the image ( projection of the centre optic on to the image plan)  %the les coordonnées de la projection du centre optique de la caméra sur le plan image 
  	%\item $s_uv$ qui traduit la non-orthogonalité potentielle des lignes et des colonnes de cellules électroniques photosensibles qui composent le capteur de la caméra. La plupart du temps, ce paramètre est négligé et prend donc une valeur nulle.\\
  	  \end{itemize}
		Once these elements are known the matrix $K$ can be designed :
	\begin{equation}
		K=
		\begin{pmatrix}
			k_u 	& 0 	& u_0 \\
			0 		& k_v	& u_1\\
			0 		&	0	& 1
		\end{pmatrix} .
		\begin{pmatrix}
			f 		& 0 	& 0  \\
			0 		& f		& 0  \\
			0 		&	0	& 1  
		\end{pmatrix} 
	\label{eq:K}
	\end{equation}
Finally, in order to estimate the positions of a 3D point in the pinhole camera reference $^C\Re$ in to the image reference $^i\Re$ 
$$
^ip=K.pr(^CP)
$$ 	  
 Parameter extrinsic (definition global) \\
  \begin{itemize}
 	\item$R_3x3$ = la matrice de rotation permettant de passer du repère lié à l'espace de travail au repère lié à la caméra\\
 	\item$t_x t_y t_z$ = les composantes du vecteur de translation permettant de passer du repère lié à l'espace de travail au repère lié à la caméra.\\
  \end{itemize}
 %Parameter utile pour notre cas de figure \\     fig : \ref{eq:V}\\
 
!!!!! the previous section can be largely (keep the basic introduction remove to be replace by ... !!!!!
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Coverage estimation in the literature}

%To estimate the coverage of a set of camera $V$  the cameras projection of each $v_i$ has to be computed individually.
 In order to compute the camera projection onto a grid the pinhole model is used with the parameters of the vector $v$, previously defined.\\
The detail to estimate the camera projection on to the floor, based on the pinhole model and the parameters ($v$), has been detailed numerous times as in \cite{193*fu2014,181*wang2017,165*jiang2010}. In \cite{193*fu2014,181*wang2017,165*jiang2010} the camera projection is used to estimate if a point $g_i$ of the grid is visible by a given camera, and that for each point of the grid. These articles handle the classic camera projection, with  the 6 Degree of Freedom (DoF) in \citep{193*fu2014} and 5 DoF in \citep{181*wang2017}). Both are used to estimate the 2D projection of the camera onto the floor (as in Figure \ref{fig:cam_projOnGrid}). \\
In  \citep{193*fu2014}, the camera projection has been computed for several rotations for all the DoF. In this case, the projection can have numerous shapes (mostly parallelogram shape).\\
In \citep{181*wang2017}, the model of camera projection begins to be simplified by assuming some fixed parameters. The fix parameters allow more efficient estimation, by economizing part of projection computation of the camera at each time. The fixed parameters allow to pre-estimate  all of part of the shape.  \\
In \citep{165*jiang2010}, the model of camera projection is used to compute one time for a fix pan and roll in order to have a coverage estimation use in a 2D map. The camera projection is finally simplified by using a kind of triangle shape.\\
To go further other model and formulation inspired by the pinhole model has been proposed as \cite{87*morsly2012,141*akbarzadeh2013,146*li2011,194*fu2010}. These models are inherited for the camera projection and adapted to fit their problems. \\
In \cite{87*morsly2012,194*fu2010} the camera is considered to be placed on the floor with a fix pan (with the looking direction almost parallel to the floor). Therefore the camera projection is simplified by an isosceles triangle where the shape depends on the focal length.\\
In \citep{141*akbarzadeh2013} the camera projection is also simplified in order to have a kind of isosceles triangle shape with considering the depth of view of the camera. In this case the  camera projection has a shape of "piece of pie" \\
In \citep{146*li2011} thanks to a fix pan and focal length, the camera projection is simplified in order to have a rectangle projection onto the ground. The sweep is designed consequently to the size of the camera projection, in order to minimize the overlap and have full coverage of the area.  

One of the common point of the present in \cite{87*morsly2012,141*akbarzadeh2013,146*li2011,194*fu2010,22*zhao2008,33*reddy2012,193*fu2014,181*wang2017,165*jiang2010}   is the computation of a camera projection on to a grid. 
The computation of the function $f()$ which has to estimate, if  a point $g_i$ of the grid is cover by a cameras $v_j$ of the network,  is not considered as really greedy (in time). But the coverage estimation using $f(g_i,v_j)$ for a point $g_i$ by a camera $v_j$ have to be done for each point of the grid and for each camera of the network in order to can estimate properly the complete area coverage.
	\begin{equation} \label{eq:sum sum v g}
		\mbox{Coverage Rate}=\sum_{i=1}^{m}\sum_{j=1}^{n}f( v_j,g_i)
	\end{equation} % for Eq Cf:87* 150*  171* voir 165*
Where $n$ the number of cameras ($v_j$) in the network; $m$ the number of point in the grid ($g_i$).  \\
%%%%%%%%%%%%%%%%%% object oclution%%%%%%%%%%%%
The solution proposed until know to estimate the camera projection onto the grid (as in Equation \ref{eq:sum sum v g})  do not take in account the occlusion made by potential obstacle.
The occlusion  made by  the obstacle has to be taking in account depending then the camera projection.
 %Some of them as \citep{165*jiang2010,181*wang2017,141*akbarzadeh2013} include the object occlusion. 
 To detected the part of the area occluded by an object, the solution commonly proposed (as is well explain in  \citep{181*wang2017}) is to check the line made between a point covered ($g_i \in Pc$) and his camera. If this line is intersected by at least by one object in the scene, the point $g_i$ cannot be considered any-more as covered. 
%  Despite the simplicity of the computation numerous operation have to be done to compute the camera projection onto the floor. \\

To take in account the potential occlusion by a obstacle $Obj_l$ the Equation \ref{eq:sum sum v g} can be extended : 
	\begin{equation}
		 \mbox{Coverage Rate}=\sum_{l=1}^{k}\sum_{j=1}^{m}\sum_{i=1}^{n}f( v_i,g_j,Obj_l)
	\end{equation} 
 
The function $f(...)$ in charge to compute a camera projection will be called for each camera, each point of the grid and each obstacle, in order to evaluate the complete coverage of the area (with the occluded part). This numerous calls will greatly increase the time computation of the coverage. It is even worst when numerous cameras projection has to be computed at each turn of a long optimisation process.\\
In this condition the efficiency of the function $f(...)$ in charge of the computation of the coverage estimation, is primordial due to the numerous calls. 
%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{landscape}	
%	\begin{table}[]
%\centering
%\caption{Sum-up ref}
%\label{tab:sum-up1}
% %              ref           |x|y |z|       tilt| |focal  coverage     nmb   
%\begin{tabular}{@{}l|p{2.4cm}    l  p{0.62cm}lp{1.3cm}p{1.57cm}p{1.5cm}p{1.6cm}p{1.3cm}p{1.2cm}@{}}
%\toprule
%\multicolumn{1}{|l|}{\textbf{Ref}}   & \multicolumn{1}{l|}{Y}              & \multicolumn{1}{l|}{Pan} & \multicolumn{1}{l|}{Tilt} & \multicolumn{1}{l|}{Roll} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}p{1.55cm}@{}}Coverage\\  room\end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Number\\ of \\ cameras\end{tabular}} & \begin{tabular}[c]{@{}l@{}}Secondary objectives \\ and constraints\end{tabular} &                      &               \multicolumn{1}{l|}{} \\ \midrule
%\rowcolor[HTML]{FFFFFF} 
%\citep{193*fu2014}   Fu 2014                                                                                                                                    & \ding{52}                                   &  \ding{52}                        & 0                         & 0                                                       & 2D                                                                                      & $\approx 10                                                                        $                                                           & barrier coverage                                                               & conection dependance &                                     \\
%\rowcolor[HTML]{EFEFEF} 
%\cite{181*wang2017}    Wang 2017                                                                                                          & 0                        & 0                         & 0                                                       & 2D                                                                                      & $\approx 600                                                                        $ & resolution                                                                     & tracking             &                                     \\
%\rowcolor[HTML]{FFFFFF} 
%\citep{165*jiang2010}   jiang 2010                                                                                                                                    & \ding{52}                                   &  \ding{52}                        & 0                         & 0                                                       & 2D                                                                                      & $\approx 10                                                                        $                                                           & barrier coverage                                                               & conection dependance &                                     \\
%\rowcolor[HTML]{EFEFEF} 
%\citep{141*akbarzadeh2013}  Akbarzadeh 2013                                                  &  \ding{52}                                   &  \ding{52}                        &  \ding{52}                         & 0                         &  \ding{52} & 2D                                                                                      & 9 to 15                                                                           & region of interest                                                              & big area             &                                     \\ 
%\rowcolor[HTML]{FFFFFF} 
%\citep{87*morsly2012}   Morsly 2012                                                                                                                    & \ding{52}                                   &  \ding{52}                        & 0                         & 0                                                     & 2D                                                                                      & $\approx 10                                                                        $                                                           & barrier coverage                                                               & conection dependance &                                     \\
%\rowcolor[HTML]{EFEFEF} 
%\cite{146*li2011}    Li 2011                             & 0                        & 0                         & 0                                                       & 2D                                                                                      & $\approx 600                                                                        $ & resolution                                                                     & tracking             &                                     \\ 
%\rowcolor[HTML]{FFFFFF} 
%\citep{194*fu2010}  Fu 2010                                                       & \ding{52}                                   &  \ding{52}                        & 0                         & 0                                                        & 2D                                                                                      & $\approx 10                                                                        $                                                           & barrier coverage                                                               & conection dependance &                                     \\
%\rowcolor[HTML]{EFEFEF} 
%\citep{22*zhao2008}  Zhao 2008                                         &  \ding{52}                         & 0                         &  \ding{52} & 2D                                                                                      & 9 to 15                                                                           & region of interest                                                              & big area             &                                     \\ 
%\rowcolor[HTML]{FFFFFF} 
%\citep{33*reddy2012}  Reddy 2012                   &  \ding{52}                        & 0                         & 0                         & fix                               & 2D                                                                                      & $\approx 10                                                                        $                                                           & barrier coverage                                                               & conection dependance &                                     \\\bottomrule
%\end{tabular}
%\end{table}
%\end{landscape}	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Coverage estimation optimization} \label{sec:coverageEstimation}
To design an efficient cost function it is necessary to reduce the  computation time and estimates the covered area . %A good estimation of the covered area  is primordial for the optimisation process. 
  The camera projection model has to be simplified with some basic assumption related to the problems.\\
Considering our case, where a camera is fixed on a UAV with a looking direction orthogonal to the ground, (without any rotation in $\alpha$ pan and $\beta$ tilt). 
In this model the camera projection is always a rectangle as shown in Figure \ref{fig:cam_proj}. In fact the camera is  placed on an UAV and have a direct look to the ground floor. The looking direction of the cameras is  perpendicular then the floor, thus the rectangle projection.
The area covered  by the camera is defined by the rectangle with : $Wr\times Hr$ 
It is possible to compute the size of the covered rectangle for a given altitude by one camera, based on these given parameters. 
%To compute the size of the projected rectangle the focal length $f$, the altitude of the cameras $A$ and the sensor size $Sw\times Sh$ is required. 
%Especially the focal length $f$, the altitude of the cameras $A$ and the sensor size $Sw\times Sh$. \\

%The altitude is the distance between the grid and the cameras, in the simple case $A$ is considered to be equal to $z$, when the grid is on the floor.  \\
Based on the useful parameters form the camera $(f ,Sw\times Sh ,z)$ the computation to estimate the size of a camera projection ($Wr$,$Hr$) is :

%\begin{equation}
%s= [Sw ; Sh]  %\mbox{   } Sw,Sh \in \mathbb{N} 
%\end{equation}
%
%\noindent Where $Sw$, sensor size width. \\
%Where $Sh$, sensor size height.

%Where $\eta$, $\upsilon$ are the horizontal and vertical camera fields of view.\\
%Estimating the width and height of the rectangle projected on the ground depend on the altitude $A$ :
%	\begin{equation}\label{eq:WrHr1}
%		\begin{split}
%    		Wr= 2\times A\times\tan \frac{\eta}{2}
%    	    \\
%    	    Hr= 2\times A\times\tan \frac{\upsilon}{2}
%    	 \end{split}
%	\end{equation} 
	
	
	\begin{equation} \label{eq:WrHr1}
	\begin{split}
			Wr=(\frac{1}{f} . Sw).z\\
			Hr=(\frac{1}{f} . Sh).z
     \end{split} 
	\end{equation}
	
	\begin{figure*}[t!]
		\centering
		\minipage{0.85\textwidth}
  		\includegraphics[width=\linewidth]{img/CamProject1Bis.png}
  
 	 	\endminipage\hfill\caption{Camera projection onto a grid. The grid is placed on the floor to discretize 	the area covered.}\label{fig:cam_proj}
	\end{figure*}

%Therefore, the size of the rectangle projected onto the floor is $(Wr,Hr)$ for the width and height. The values of $Wr$ and $Hr$ are directly linked to the altitude of the camera. 


%In the simple case and in the initialization, $A$ is equal to $z$ and $z$ is selected in a range given by the UAV or the user. \\
%Once the couple $(Wr,Hr)$ as been computed for a $A=z$ it is easier to change the altitude of the camera. The rectangle projection will be affected in proportion.
%	\begin{equation}\label{eq:A.coef}
%		\begin{split}
% 		   	f(A)= (Wr,Hr) \text{ based on eq \ref{eq:WrHr1}}\\
%    		f(A.Coef)=f(z)= (Wr.Coef,Hr.Coef)      
%    	 \end{split} 
%	\end{equation}
%Therefore, to any altitude $z$ is existing $A.Coef$ where the size of the camera projection onto the floor is $(Wr.coef, Hr.coef)$. Thanks to this the eq: \ref{eq:etaUpsilon} and eq: \ref{eq:WrHr1} have to be compute once  for a given focal length $f$ and sensor $s$ with an simple $A=z$. Adding simple $Coef$ the size of camera projection can be easily simplified in order to limit the useless computation (only 2 multiplication instead then equation \ref{fig:cam_proj} and \ref{eq:etaUpsilon}.

When $f$ represent the focal length, $z$ the altitude of the cameras and  $Sw$, $Sh$ the sensor size.
%The model of camera projection is greatly simplified by the UAV assumption (fixed pan, tilt and focal length).
Thanks to this model of camera projection as a simple rectangle, the size of the rectangle projection is directly related to the altitude. The altitude is assuming to be a coefficient of the Equation \ref{eq:WrHr1}\\
%The simplification by modelling all the cameras with a fix orientation and same ability (in order to have a rectangle projection onto the ground) 
All these simplifications helps the cost function to be fast and efficient.  In fact in the Equation \ref{eq:WrHr1} the parameters focal length and the sensor size are fixed and do not need to be recomputed for each camera position only the altitude $A$ has to be updated.\\

\subsection{Parameters to optimize }\label{sec:parameterToOptimize}
% en raison de    ref 191*
By dint of the simplification presented previously the parameters vector (see Equation \ref{eq:v}) can be reduced.
The computation of one camera projection onto a grid (as in Section \ref{sec:GridMap}, and the camera is in altitude with a looking direction orthogonal to the floor. Based on the Equation \ref{eq:WrHr1}
Just few parameters are necessary as showed in Equation \ref{eq:WrHr1}. Thanks to the proposed assumption,  Equation \ref{eq:v} can be reduced by keeping only the position of the camera $(x,y,z)$ and the roll ($\gamma)$ as:
	\begin{equation}\label{eq:v2}
		v=(x,y,z,\gamma ) 
	\end{equation}

In our case the roll $\gamma$ can take different state, portrait or landscape with a rotation of $0^\circ$ or $90^\circ$.
Reducing the number of  parameters, passing to the equation \ref{eq:v} to \ref{eq:v2} are to reduce the number of parameters to optimize. 

 % The reduction of the number of parameters will greatly affect the optimisation process.
 %Indeed, in addition to reduce the time computation, this simplification reduce the number of parameters to optimize.   

Until now, the camera projection estimation was addressed with only one camera. But we want to compute the coverage for a set of cameras. 
The solution is based on the previous estimation for the camera projection, but adapts to the location of each camera. By positioning the rectangle projection to have the center of it at the $x$ and $y$ position and compute the occupation grid.

In order to win a bit of time, each point of the grid already covered by a camera are not tested for the next cameras. %This small modification will impact positively the computation time for the coverage estimation of a network of cameras. 
That mean in the equation \ref{eq:sum sum v g} the value of $m$ decrease as $i$ increase. More exactly the size of $m$ decrease as the area coverage increase.  


\subparagraph{Parameters representation. \\}

To represent the problem we need a set of cameras. The precedent notation can be extended to have a set $V$ of $n$ cameras defined according to:

\begin{equation}\label{eq:V}
		\begin{split}
			V= {v_1,v_2,...,v_n} \mbox{  , } n\in \mathbb{N}^*
				\\
			\mbox{ with } v_i= (x_i,y_i,z_i,\gamma_i)
		\end{split}
	\end{equation}
	
%	\begin{equation}\label{eq:V}
%		\begin{split}
%			V= \{v_i\} \mbox{  , } \forall i=[1;n] \mbox{ , } n\in \mathbb{N}^*
%				\\
%			\mbox{ and } v_i= (x_i,y_i,z_i,\gamma_i)
%		\end{split}
%	\end{equation}
\noindent Where $n$ is the given number of cameras in the network. The coordinate of a camera $v_i$ with are the $i$th camera of the network is defined  with $x_i, y_i, z_i,$ in a given room and $\gamma_i$ the roll rotation (portrait or landscape). The parameters not contained in $V$ and used to compute the cameras projection are  identical for all the set $V$, and are fixed at the beginning of the optimization.\\
Therefore, $V$ represents a solution. $V$ contain all individual positions and orientations of the set of cameras for a predefined focal length, sensor size and related map depending on the problem.% wish also include the potential constraint as  some restriction on the map.\\
 Obviously all the solution $V$ are not a "possible solution" for our problem. Some solution $V$ does not respect the set of constraint noted $E$. E represent the set of constraint ( the set E is defined  more in detail latter). 

In addition the solution $V$ should respect a set of constraints  $E$ (see Eq.\ref{eq:Vs}). Among the constraint few of them was already discussed, as the occlusion, the map restriction, the k-coverage, or some constraint more specific to the problems (as saw in chapter \ref{chap:stateOfTheArt}).\\
 The "possible solution" $Vs$ must take in consideration with the set $E$ as :

%\begin{equation}\label{eq:Vs}
%Vs=V \mbox{ , iff } E(V)=\begin{cases} 1 & \mbox{iff } E_i(V)=1 \mbox{ , with } i=1...Nc \\ 0, & \mbox{otherwise} 
%\end{cases} 
%\end{equation}
 \begin{align}\label{eq:Vs}
Vs=V \mbox{ , iff } E(V)= \begin{cases} 1, & \mbox{iff } E_i(v)=1 \mbox{ , with } i=1...Nc \\ 0, & \mbox{otherwise}  \end{cases}
%\\ G'=G-U \mbox{ ,    }  \mbox{ }U= \{ g_i | g_i \in G, g_i \mbox{ are the non intresting points} \}
\end{align}

Where $E_i(V)$ is the function applied to verify  the $i$th constrains of the set $E$ on the solution $V$. $Nc$ is the number of constraints needs to be satisfied to have an acceptable solution.
That mean among all the possibles combination of parameters $V$ only the one intersect the set of the constraint $E$ are an possible solution.  If we are considering  all the $V$ and all the $E$ as two subset $Vs$ is defined as $Vs=V\subset E$. 

The problem of monitoring an area and more specifically the problem of area coverage may contain many constraints depending of the environment and the context. As example: the room shape, minimizing  the altitude,  have the best resolution, orientation of the camera, the possible occlusion,... All this constraints are included in the set $E$. 
The constraints have to be defined depending on the problem and the goal.

\section{Cost Function}
The cost function has to evaluate the quality of a given solution. To estimate the quality of a solution one of the main criteria is the coverage rate. The precedent section has been focus on the computation of the coverage rate. \\
To create an efficient cost function, other criteria has to be taken in to account as the constraints.  
 To establish the cost function a list constraint has to be done. Each constraint of this list does not have the same importance. In order to split the constraints depending then their impact on the problem two types of constraints are introduced in the  following sections before to discuss about.
 

\subsection{Constraint list}\label{sec:constraint}

%Make the exhaustive list of all the constraints for the problem of cameras positioning is not really interesting and almost impossible.
 The constraints can be numerous and depend mainly of the problem formulation and the context. Like that few of them was briefly introduced in the previous section as in chapter \ref{chap:stateOfTheArt}, section \ref{sec:coverageEstimation},... This part is focus on the list of the constraints used in our case and detail their design.

%The constraint considered in this work are : 
%\begin{itemize}
%	\item Fix number of the cameras.
%	\item Fix parameters of the camera.
%	\item No rotation  on  $\alpha$ and $\beta$ (pan and tilt).
%	\item Possible fix altitude.
%	\item Altitude boundary (not too hight, not too low).  	
%	\item The map boundary (rigid rectangle).  
%	\item Non rectangles map with possible holes.
%	\item To have some fix cameras in the set. 
%	\item The resolution.\\
%
%\end{itemize} 
\subparagraph{Fixed number of the cameras}
One of the first constraint is the fixed number of cameras. This constraint as some others (detailed latter) are  useful to simplify and restrict the possibility of the problem. This constraint permits us to focus on the fine optimisation (as in \citep{22*zhao2008} where both are tested). The number of cameras is fixed at the beginning of the optimisation and no more camera will be added during the optimization process.  

\subparagraph{Fixed parameters of camera and no rotations}
Fixed parameters of the cameras and no rotations ($\alpha$ and $\beta$) has been introduced previously (section \ref{sec:parameterToOptimize}). These constraints imposed by the use of an UAV are also an advantage for the optimization by simplifying the coverage estimation and limit the number of parameters to optimize. The parameters are fixed at each beginning of the optimization. Thanks to this constraint the optimization  has to focus on the precise  cameras positioning.

\subparagraph{Fixed altitude}
 The fixed altitude is a constraint use in order to limit the number of parameters to optimize. The use of this constraint is used to reduce the difficulty by reducing the possible search space (see section \ref{sec:OptimizationComplexity}). It is also useful for other assumptions, as a cameras on the celling or for a submarine  \cite{66*galceran2013}. This constraint is an optional constraint and is not always used in the experiments presented in the following sections.   

\subparagraph{The altitude boundaries}\label{sec:altitudeBoundary}
 When the altitude is not fixed some limits must be chosen to avoid the extremely high and low altitudes. The highest altitudes will be fixed depending on the UAV ability and other restrictions as the laws. The lowest altitude has to be fixed for the safety of the users under the UAV.  
In practice the  altitude boundary is defined with :
 \begin{equation}\label{eq:boundaryZ}
   \inf z\leq z\leq \sup z  
 \end{equation} 
 Where $\sup A$ is the maximal altitude of the camera and $\inf A$ the minimum altitude. $A$ is the altitude between the camera and the grid. %$A$ is the fix altitude of a camera where the camera projection has been computed with $A=z$ with only $coef$ vary as introduced in Equation \ref{eq:A.coef}. 
 
 \subparagraph{The map boundaries} \label{sec:MapBoundaries}
The map boundaries, is a constraint similar then the altitude boundary. Despite the shape of an area to cover some maximum boundary can be made. In fact for any shape as complex as it, it is  possible to encapsulate in a rectangle. The rectangle map boundary is defined by a width $W$ and height $H$. The boundary on $x$ and $y$ are:
 \begin{equation}
  \begin{array}{lcl}
  	0\leq x\leq W \\
  	 0\leq y\leq H 
  \end{array} 
 \end{equation}  
 
By associating the altitude boundaries (from Eq\ref{eq:boundaryZ}), a cube boundaries limits the position of the camera in the 3 dimensional spaces. 
\begin{equation}\label{eq:3dBoundary}
  \begin{array}{lclcl}
  	0\leq x\leq W \\ 0\leq y\leq H  \\ \inf z\leq z\leq \sup z  
  \end{array} 
 \end{equation} 
 
 \subparagraph{Non rectangle map with possible hole} \label{subPara:MapConstraintAndObstacle}
The map to cover can be much more complex than a simple rectangle and may take any shape with can be composed by holes. The Figure \ref{fig:boundaryMap} illustrates the map complexity.% The black part of the map are the zone with are out. That mean these sub-parts have not interest to be covered.
 \begin{figure}[t!]
 \begin{center}
\minipage{0.85\textwidth}
   \includegraphics[width=\linewidth]{img/BoundaryMap2.png}
  \caption{Map to cover with the map boundary in red (W and H size) in black the sub-part have no interest to  be covered.   }\label{fig:boundaryMap}
  \endminipage\hfill
  \end{center}
\end{figure}
To take into account the complex shape map % the gird has been designed with removing some of this points.
the grid $G$ is reduced in order to keep only the points in the white sub-part. In the example (see Figure \ref{fig:boundaryMap}) each white pixel of the map is a point of the grid to cover. 
Concretely this implementation by using a grid map has some advantage as  the flexibility of the grid customization . \\
%Among the advantage, the flexibility of the grid customization. 
In addition, it allows the optimization to test some exotic solutions, as allowing the camera position on the black sub-part or in a border of it. Obviously the exotic solution with a camera position on the black side does not increase the coverage rate  but with a correct converge  the camera are not  covering the black sub-part of the map (or small part of it). The interest of leave this freedom to the optimization is double, first control the quality of the the solution obtained, the second is the variety introduced during the exploration of the black sub-part of the map. 

%
%\subparagraph{Some fix cameras in the set}
%Having some fix cameras position in the set of cameras, is an optional constraint. This allows to have few manual cameras position from an user or other algorithms.
%One case can be to have some specific area as the entrance where have to be surveyed by a cameras dedicated to.  
%To implement this constraint the solution applied during the experiments (presented later) is to adapt the map by removing the point of the grid $G$ cover by the sub-set of fix cameras (as if these points of the grid was covered).

\subparagraph{Resolution}
The resolution of the images is related then the sensor size (in px) and the distance between the camera and the object. In our case, the sensor size is dependent on the properties of the camera mounted on the UAV.% and the object filmed is the floor of the area to cover.
 The distance between the camera and floor (or the object) is the altitude as  defined in the Section \ref{sec:altitudeBoundary}. \\
The resolution constraint has to maximize the resolution. In order to maximize the resolution during the optimization the altitude criteria is modified in order to be as lower as possible.\\
Considering only the minimisation of the average altitude of the cameras is harmful for the coverage.
Consequently, during the optimization process a trade off between the altitude (and the related resolution) and the coverage rate have to be done.
 The altitude of the cameras  as to be  minimized, the average of the altitude is used as index estimate the resolution  %($\min{\frac{\sum^n_{i=1}{z_i}}{n}}$ )
 . In order to manage this trade off, the average altitude of the cameras is included in the cost function  and has to be minimized (see section  \ref{sec:costFun}).  \\
 
\subsection{Constraint types} \label{sec:ConstraintTypesSoftHard}
 
Among the constraints listed different priorities and restrictions exist. Indeed the constraint can be considered as hard or soft constraint. %The two sub-class are the hard constraint and the soft constrain  presented in the following paragrapher.

\subparagraph{Hard Constraint}
 %Some of the constraints presented are called "Hard constraint". \\
 The hard constraints limit the possible solution by do not allowing the solution with does not respect it. These hard constraints is directly used during the optimisation process to prohibit any solution to be out of these boundaries. These hard constraints has to be integrate in the optimization process in order to cannot generate a solution with does not respect it. Consequently the hard constraints may slow down the generation of the individuals due to the specific generation and required test.\\ 
 For example, the 3D boundary as defined in Equation \ref{eq:3dBoundary} is a hard constraint. Improving that, each camera must be inside the 3D boundary.\\ %Otherwise  if one of the camera are out all the solution must be rejected. 
 
 \subparagraph{Soft Constraint} 
 %In the other case, some constraint can be considered as "Soft Constraint".\\ 
 The soft constraints have to minimize a set of errors. 
A solution that does not fully respect the soft constraint can be considered as acceptable if the
% If a soft constraint is not fully respected the solution can be considered as acceptable and this small
 amount of error does not affect so much the final answer. The soft constraint is assimilate to acceptable error. \\
 The soft constraint leaves the possibility during the optimization process to allow some mistakes in order to learn about it.  
 If the soft constraint is noted $\epsilon$ and the hard constraint are noted $ \varepsilon '$ like that the constraint set is $E=\epsilon+\varepsilon'$.\\
 
% \begin{equation}
% 	\max f(Vs) -\epsilon  \mbox{  } \forall Vs \subset \varepsilon'
% \end{equation}
  \begin{equation}\label{eq:constraintEpsilon}
 	\max f(Vs) - \min \epsilon  \mbox{  } \forall Vs \subset \varepsilon'
 \end{equation}
 
The objective is to maximize the coverage of a set of cameras ($f(Vs)$) with respect the hard constraints $\varepsilon'$ and minimized the error form the soft constraints $\epsilon$.
Concretely the soft constraints are commonly integrated in the Cost Function as can be the resolution or  the  complex shape of the map thanks to the grid design. 


\subsection{The cost function implementation} \label{sec:costFun}

The cost function has the mission to estimate the quality of an answer. In our case, an answer is the position  of a set of cameras. The cost function is essential in the process of optimization as that was introduced in the Section \ref{sec:CostFunctionGA}.

The cost function has to estimate the area cover by a set of cameras in order to do that the area is discretized by a grid as in Section \ref{sec:Grid}. \\
The grid customization permit to introduce some of the soft constraints as the complex shape of the map by removing the points out of the area to cover.  \\
The grid modification  allow the cameras position to cover the area already covered and removed form the grid. The consequence of it are to reduce the coverage rate possibility. The optimization have to minimized this error.

To evaluate the coverage of a set of cameras is essential to can estimate the cameras projection of each, as detailed in the Section \ref{sec:CamerasDefinition}. The area cover by the $j$-Th camera is noted as in Equation \ref{eq:PciK} (where $Pc\in G$). By iteratively repeated this for each camera of the set the full area coverage is computed (as Equation\ref{eq:sum sum v g}). 

Based on, the simplest cost function is the coverage estimation.
\begin{equation}\label{eq:CostFBase}
C(Vs) =  \frac{\sum_{i=1}^N{Pc_i} }{m}   
\end{equation}
Where $N$ is the number of cameras; 
$m$ represent the number of points needed to describe the grid $G$ (as in Equation \ref{eq:Grid}); $Vs$ is the solution with respect the hard constraints.
The cost function $C(Vs)$ give the quality of the solution $Vs$.

This version of the cost function $C(Vs)$ does not take in account the resolution constraint. The resolution is  strongly linked with the camera altitude $z$ (as show in Section \ref{sec:altitudeBoundary}).
 A criteria must be added in the cost function formula of the Equation \ref{eq:CostFBase}. The average of the altitude $z$ is used and have to be included in the cost function.
 \begin{equation}\label{eq:CostFResolutionPart1}
\overline{z}= \frac{\sum_{i=1}^N z_i}{N}     
\end{equation}

 If the resolution is strongly related then the altitude the average of it $\overline{z}$ can be considered as a part of the soft constraint ($\epsilon$)  in the Equation \ref{eq:constraintEpsilon} and the Equation \ref{eq:CostFBase} may be updated as : 
 
 \begin{equation}\label{eq:CostFResBase}
C =  \frac{\sum_{i=1}^N {Pc_i}}{m}  - \frac{\sum_{i=1}^N z_i}{N}     
\end{equation}
 
The Equation \ref{eq:CostFResBase} is used in the cost function to add the resolution constraint. Consequently, the optimization  try to minimize the average altitude and maximize the coverage with no priority. Concretely by just applying this Equation  \ref{eq:CostFResBase} the optimization will first minimize the average altitude by positioning all the cameras at the minimum altitude (with respect the hard constraint of altitude boundary) and in second time try to maximize the position (on $x$ and $y$) of the cameras. 

In order to have a priority  between the coverage and the altitude a weigh has to be made on the Equation \ref{eq:CostFResBase}. The weigh have to be chosen carefully.
The weight has to be auto-adaptable depending on area covered. In order to give more priority to the coverage when the coverage rate is low and add importance to the resolution when the area is already well covered. The coverage has to stay the priority the resolution must be optimized in a second time.
The best solution, is to link the weight of the resolution criteria with the coverage rate.

\begin{equation}\label{eq:CostFResPondere}
  \sigma \times \sum_{i=1}^N {Pc_i} \times \frac{\sum_{i=1}^N z_i}{N}     
\end{equation}
Where $\sigma$ is a weighting coefficient at 0.06 to reduce the priority on the resolution criteria. 
Based on it the final cost function is: 

\begin{equation}\label{eq:CostF}
C(Vs) =  \frac{\sum_{i=1}^N{Pc_i}  - \delta  \times \sum_{i=1}^N {Pc_i} \times \frac{\sum_{i=1}^N z_i}{N}  }{m}   
\end{equation}

Thanks to this formula, a proposed answer $Vs$ can be evaluated and returns the quality of the solution for the problem of the coverage maximisation and the minimization of the altitude in the second time. The cost function integrate all the soft constraint either by the design of the grid or by the formula of the cost function $C(...)$.

The cost function presented is the final one, but the building of it was an incremental work and numerous version was test in term of weight, priorities, and constraints. The one presented here is the more equilibrated.\\ 
Despite that some of the work  presented in the following sections are made with the basic cost function from the Equation \ref{eq:CostFBase}. In this case, the element which compose $v$ can be reduced as only $v=(x,y,\gamma)$ depending then the need of the experimentations.\\

The final and complete cost function $C(Vs)$ have as input a vector $Vs$ with are composed by all the cameras position and orientation of the network. It is also composed by the map of the area to cover. Where $G$ include the soft constraints as the room shape. The constraint of resolution is added by using the average altitude in the equation \ref{eq:CostF}.
The value is returned by the cost function $C(Vs)$ is the quality of a solution to our problems of coverage using an UAV.






%%The use of an UAV to cover an area allow the simplification of coverage computation as well as the numbers of parameters to optimize.
%% !!!!
%%Thanks to the simplification of the problem due to the used of an uav the parameters to optimize can be reduced. (as in eq: \ref{eq:v}
%%Thanks to the simplification the element to optimize for an efficient coverage can be limited to pass from Eq. (\ref{eq:v}) to  Eq.(\ref{eq:v2}) as shown: 
%%\begin{equation}\label{eq:v2}
%%v=(x,y,A,\gamma )
%%\end{equation}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%fin de la camera pin hole explication%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
%    
%In the following part a vision sensor is employ to acquire the image of area to control. The vision sensor may be a common camera with different parameter as:
% \begin{itemize}
% \item Every camera has a position  in  $(x, y, z)$;
% \item Every camera  has an orientation composing to 3 degrees of freedom  pan$(\alpha$),  tilt$(\beta)$, roll$(\gamma)$, see Figure \ref{fig:PanTiltRoll}.
% \item Every camera have different optical parameters and the helpful to take in consideration for coverage estimation is the focal length $f$ and the sensor size $s$(based on the formulation of Chrysostomou et al[82]). 
% \end{itemize}


 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \section{Optimization complexity and search space} \label{sec:OptimizationComplexity}
 
%To cover an area with a certain amount of sensor many solutions can be possible.
In spite of the simplification presented before, the problem stays complex. There exist many positions for each camera to cover an area with a certain amount of its. This number of position can be estimated for each camera as follows.\\   
Each camera defined by the position on $x, y, z $ and $ \gamma$ can be set anywhere in the search space of a cameras named $Sp$ : 
\begin{equation}\label{eq:SearchSpace}
 Sp=(W\times H \times ( \max(z)-\min(z)) \times 2 )  
\end{equation}
Where $W$ and $H$ are the size as width and height of the area to cover, $\max(z)-\min(z)$ is the range of possible altitude. Two is to define the roll $\gamma$, as the rectangle projection is horizontal or vertical (landscape or portrait).% $Sp \in E$ The search space of each camera $Sp$ allows to take in consideration some strong constraint $E$ like the boundary of the area or the restriction in the degree of freedoms.

The problem of the search space is the propensity to increase rapidly as the area grows. This phenomena is accentuated by the number of cameras $n$.
%It is possible to express the size of the search space as a succession of product for estimate the number of feasible position on $x, y, z $ and $ \gamma$ for each combination of the set cameras : 
%\overset{N}{\underset{Sp}{e}
 \begin{equation} \label{eq:Combinaison}
 \begin{pmatrix} n \\ Sp \end{pmatrix}  = \frac{Sp!}{n!(Sp-n)!} = |Vs|
 \end{equation} 
 Where $|Vs|$ is the number of possible solution for a set of $n$ cameras in the worst case. %$|Vs|$ is the 
 %  \begin{equation} \label{eq:SearchSpace}  \sum_{n=1}^N \sum_{x=1}^{W} \sum_{y=1}^{H} \sum_{z=1}^{)} \sum_{\gamma=1}^{2}Pc_i  - \sum_{k=1}^{E}Pc_i=Vs  \end{equation}
 In fact the size of the search space of one camera ( as Eq. \ref{eq:SearchSpace}) associate to the set of $n$ cameras (as Eq.\ref{eq:Combinaison}) make an exponential number of possible solution depending mainly then the size of the area and the number of the cameras in the network. \\
 Obviously in view of this behaviour the use of a deterministic solution based on a heuristic does not seem to be a good answer to have an efficient solution. In addition the number of possible solutions $|Vs|$ makes the computation of an optimal almost impossible due to the numerous local minima. 
 The formulation of the problem is primordial in order to reduce the size of the search space and give a chance to the optimization to converge.
 The complexity for the problem of camera positioning is  at least  NP-hard has  the AGP. In fact if we considering the  camera positioning  as inherited  from AGP  with some extra constraints (as field of view and depth of field). Also the literature is unanimous about the  NP-hard complexity as discussed in  \citep{43*erdem2006,53*packer2008,62*vijayan1991,22*zhao2008,49*ning1994}. %43* 53* 62*  49* 22*].100*

The size of the search space is even more important then the method applied. In fact due to the  problem complexity   and  the numerous local minima the classic optimization algorithms  is not  appropriate. The stochastic algorithms  are efficient in this case.  Due to the wide  search space that increase the difficulty to converge. The stochastic algorithms is  widely based on the optimized random solutions and bigger is the  search space,  harder is to get the best solution. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%% FIN coverage estimation%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


 
